{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro / Background\n",
    "Three types of data science role\n",
    "1. Analyze user behavior to improve a product\n",
    "2. Consultative\n",
    "3. Research / algorithm development\n",
    "\n",
    "Joe's role at SignalFx is a mix of 2 & 3. Examples of tasks at SignalFx:\n",
    "1. Detect anomalies in time series\n",
    "2. Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods\n",
    "Goal: Factor a similarity measure as a composition of a feature map followed by an inner product.\n",
    "\n",
    "An *inner product space* is a vector space $V$ equipped with an inner product $\\langle , \\rangle$ that is bilinear, symmetric, and positive definite ($\\langle x, x\\rangle \\geq 0$ with equality iff $x=0$).\n",
    "\n",
    "A *kernel* on a set $X$ is a similarity measure $k:X\\times X\\to \\mathbb{R}$ that admits a  factorization $X\\times X \\to^{\\Phi\\times\\Phi} V\\times V \\to^{\\langle,\\rangle} \\mathbb{R}$. (Turns out to be equivalent to positive definiteness of similarity matrix.)\n",
    "\n",
    "Think of first arrow as data modeling and second arrow as linear algebra / inner product.\n",
    "\n",
    "It's possible that $X$ is already an inner product space, but $\\Phi$, usually called the feature map, is non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Geometric Interpretation of a Classification Algorithm\n",
    "A classification problem is a special type of supervised learning where the target labels (what you're trying to predict) are categorical. (A regression problem is where you are trying to predict continuous labels.)\n",
    "\n",
    "Suppose you have data $\\{(x, y)\\,\\,\\vert\\,\\, x\\in X, y\\in \\{\\pm 1\\}\\}$, where $X$ is the data and the $y$ are labels and you have a kernel $\\Phi: X\\to V$ and you want to classify a new data point. Here's an algorithm.\n",
    "\n",
    "Let $$c_+ = \\frac{1}{n_+}\\sum_{y = 1}\\Phi(x_i)$$ and \n",
    "$$c_- = \\frac{1}{n_-}\\sum_{y = -1}\\Phi(x_i)$$. These are the means of the classes under $\\Phi$. Make the classification rule that a new point $x$ goes to the class whose mean it is closer to. Working that out:\n",
    "\n",
    "$$(d(\\Phi(x), c_+))^2 = \\langle\\Phi(x) - c_+, \\Phi(x) - c_+\\rangle$$\n",
    "So $$(d(\\Phi(x), c_+))^2 \\leq (d(\\Phi(x), c_-))^2$$ if and only if\n",
    "$$\\langle\\Phi(x), \\Phi(x)\\rangle - 2\\langle\\Phi(x), c_+\\rangle + \\langle c_+, c_+\\rangle\n",
    "\\leq \\langle\\Phi(x), \\Phi(x)\\rangle - \\langle2\\Phi(x), c_-\\rangle + \\langle c_-, c_-\\rangle$$\n",
    "if and only if\n",
    "$$ - 2\\langle\\Phi(x), c_+\\rangle + \\langle c_+, c_+\\rangle\n",
    "\\leq - 2\\langle\\Phi(x), c_-\\rangle + \\langle c_-, c_-\\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $(d(\\Phi(x), c_+))^2 \\leq (d(\\Phi(x), c_-))^2$ if and only if $ 2\\langle\\Phi(x), c_+\\rangle - 2\\langle\\Phi(x), c_-\\rangle + \\langle c_-, c_-\\rangle - \\langle c_+, c_+\\rangle$ is non-negative. The classification rule is then the sign of $ 2\\langle\\Phi(x), c_+\\rangle - 2\\langle\\Phi(x), c_-\\rangle + \\langle c_-, c_-\\rangle - \\langle c_+, c_+\\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume that the two classes are equally likely, $\\langle c_+, c_+\\rangle = \\langle c_-, c_-\\rangle$, and that $k(\\cdot, x_0):X\\to \\mathbb{R}$ is a density for all $x_0\\in X$. (Really the assumption is that you've normalized/sampled the data to make this true.)\n",
    "\n",
    "Estimate that $p_+(x) = \\frac{1}{n_+} \\sum_{y=1}k(x, x_i)$. Then \"assign x to the sign of $\\langle\\Phi(x), c_+\\rangle - \\langle\\Phi(x), c_-\\rangle$\" is Bayes' rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Rule: $$P(y = 1\\,\\vert\\,x) = \\frac{P(x\\,\\vert\\,y=1)P(y=1)}{P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of Reproducing Kernel Hilbert Space\n",
    "Goal: given any positive definite similarity measure (i.e. such that $k(x_i, x_j)_{ij}$ is positive definite), represent it as a kernel on functions of the original space $X$.\n",
    "\n",
    "Define\n",
    "$\\Phi:X\\to Fun(X, \\mathbb{R})$ by $x\\mapsto k(x, \\cdot)$. Define an inner product on the image of $\\Phi$ in $Fun(X, \\mathbb{R})$.\n",
    "\n",
    "Take the $\\mathbb{R}$-linear span of the images $\\Phi(x_i)$, so an element looks like $f(\\cdot) = \\sum \\alpha_i k( ,x_i)$. Let $g(\\cdot) = \\sum_j \\beta_j k (, x_j)$.\n",
    "\n",
    "Define an inner product $\\langle f(\\cdot), g(\\cdot) \\rangle = \\sum_{i, j} \\alpha_i \\beta_j k(x_i, x_j)$. This is well-defined, symmetric, bilinear, satisfies $\\langle f, f \\rangle \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claim: $\\langle f, f\\rangle$ = 0 implies $f=0$.\n",
    "\n",
    "Proof: $\\langle k(\\cdot, x), f\\rangle = f(x)$ \n",
    "(Aside: $\\langle k(\\cdot, x), k(\\cdot, x^\\prime)\\rangle = k(x, x^\\prime)$ is why it's called reproducing.)\n",
    "By this observation and the Cauchy-Schwartz inequality $\\vert f(x)\\vert^2 = \\vert\\langle k(\\cdot, x\\rangle, f\\vert^2 \\leq k(x, x)\\langle f, f\\rangle$. If the right hand side is 0, the left hand side is 0. $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties/example (Gaussian kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Positive definite) kernels are closed under multiplication, non-negative linear combinations, and limits. In particular, if $X$ with $\\langle, \\rangle$ is an inner product space, then $x, x' \\mapsto \\exp (\\langle x, x' \\rangle / \\sigma^2)$ is another (PD) kernel. Note also $X \\to \\mathbb{R}$ defined by $x \\mapsto \\exp( - || x ||^2 / 2 \\sigma^2)$ is a feature map with target an inner product space, so $X \\times X \\to \\mathbb{R}$ defined by $x, x' \\mapsto \\exp( - || x ||^2 / 2 \\sigma^2) \\exp( - || x' ||^2 / 2 \\sigma^2)$ is a (PD) kernel.\n",
    "\n",
    "Therefore we may define the Gaussian kernel via $\\exp( - || x - x'||^2 / 2 \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: If you have a kernel whose matrix is positive definite, then there exists a $\\Phi$ and an inner product space satisfying the factorization diagram from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representer theorem\n",
    "Theorem: Let $\\Omega: [0, \\infty)\\to\\mathbb{R}$ be an increasing function, such as multiplication by a positive number $\\Omega(t)=\\lambda t$ where $\\lambda > 0$. Let $X = \\{ x_1, \\ldots, x_n \\}$ be a set with feature map to an inner product space of functions on $X$. Let $c$ be a loss function $c:(X\\times \\mathbb{R}^2)^n\\to \\mathbb{R}\\cup\\infty$ (i.e. minimizing this function gives the solution to an optimization problem that gives parameters for some machine learning problem). Then every local minimizer $f$ of $$c((x_1, y_1, f(x_1)), \\ldots, (x_n, y_n, f(x_n))) + \\Omega(\\vert\\vert f\\vert\\vert^2)$$ admits a representation of the form $$\\sum_{i=1}^n\\alpha_ik(\\cdot, x_i)$$ (sum of the kernel on points in the training set).\n",
    "\n",
    "Example cost function is a sum of terms $f(x_i) - y_i$, maybe squared. The $c$ term of the objective function measures the error on the training set. The $\\Omega$ term encourages the solution to have a small norm; this can be viewed as a complexity penalty.\n",
    "\n",
    "This is important because it says that if we are trying to solve an optimization problem in a high-dimensional feature space, then the solution can be expressed in terms of the kernel evaluated on the sample points.\n",
    "\n",
    "Proof uses direct sum decomposition of feature space into image of sample points under $\\Phi$ and its orthogonal complement. Show the objective function can only decrease when you replace an input $f$ with its projection onto the image of sample points under $\\Phi$. You will need to use $f(x_i) = \\langle k (\\cdot , x_i), f \\rangle$, and $a = a_0 + a_1$ implies $\\vert \\vert a_0 \\vert \\vert \\leq \\vert \\vert a \\vert \\vert$ for an orthogonal decomposition of $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Support Vector Machine\n",
    "Suppose $X$ is $\\mathbb{R}^2$ and points labeled $\\pm 1$. Goal of the SVM is to separate these points with a hyperplane. The optimization problem we solve to do this is to find the hyperplane with the maximal margin (distance from the hyperplane to the closest class). Write hyperplane as $w \\cdot x - b = 0$. Want class $1$ to be on $w \\cdot x - b \\geq 1$ side, class $-1$ to be on $w \\cdot x - b \\leq -1$. Margin (distance between $w \\cdot x - b = 1$ and $w \\cdot x - b = -1$) is $2 / \\vert \\vert w \\vert \\vert$. Maximizing this is equivalent to minimizing $\\vert \\vert w \\vert \\vert$ (or its square) subject to $y_i (w \\cdot x_i - b ) \\geq 1$.\n",
    "\n",
    "We are assuming that the data is linearly separable -- we'll make this true with a kernel anyway if it's not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are these other kernels doing?\n",
    "Solving the same SVM optimization problem, but in a different space. Replace the usual linear structure on $\\mathbb{R}^2$ with the structure of polynomial functions on $\\mathbb{R}^2$ in the case of the polynomial kernel or (implicitly) the space of functions on $\\mathbb{R}^2$ in the case of the Gaussian kernel.\n",
    "\n",
    "Replace a point with a \"bump\" supported near it. The \"bump\" is really a function on the space $X$. Solve a linear problem in the space of functions to get a decision boundary which geometrically looks like it's composed of a bunch of bumps.\n",
    "\n",
    "The \"support vectors\" are those closest to the support hyperplane. They are the sample points appearing in the representer theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do you choose $\\sigma$?\n",
    "Good starting choice for $\\sigma$ would be standard deviation of all the non-negative distances between points. Then you would look at nearby values to see if they improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if you have more than 2 labels?\n",
    "There are many standard ways to turn a multi-label problem into a collection of two-label problems and aggregate the results. For each class, formulate a one-vs-rest problem. Take the class with the strongest prediction. NB imbalanced classes may require special attention (already true in the two-class case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
