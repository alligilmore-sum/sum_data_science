{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 - Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: data often arrives with the wrong coordinates and/or too many dimensions (features).\n",
    "\n",
    "Example: image data represented as gray scale vector. May not need all the pixels to characterize an image. Pixel coordinates also not so meaningful, may prefer features describing interpretable aspects of an image (edges, facial features, ...).\n",
    "\n",
    "There are many dimension reduction algorithms. We will talk about 4. They differ in what geometric structure they want to preserve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (principal components analysis)\n",
    "\n",
    "trying to preserve directions of maximal variance\n",
    "\n",
    "Theorem. The following (definitions of PCA) are equivalent.\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times p}$ be a data matrix ($n$ examples, $p$ features). Assume the columns are mean-centered.\n",
    "\n",
    "(1) the principal components are:\n",
    "$v_1 \\in \\mathbb{R}^p$ (unit vector) that maximizes the variance of projecting $X$ onto $v_1$\n",
    "\n",
    "In general, $v_i$ maximizes the variance of projecting $X - proj_{v_1}(X) - \\cdots - proj_{v_{i-1}}(X)$ onto $v_i$, and $v_i$ is orthogonal to the span of $\\{ v_1, \\ldots v_{i-1}\\}$.\n",
    "\n",
    "(2) the principal components are eigenvectors of the covariance matrix $Cov(X) (\\approx X^T X \\approx \\mathbb{E}((x_i - \\mu_{x_i}) \\cdot (x_j - \\mu_{x_j}))$ of $X$\n",
    "\n",
    "(3) $v_1, \\ldots , v_p$ are the right singular vectors of $X$. A singular value decomposition of $X$ is a product $X = U \\Sigma V^T$ with $U, V$ orthogonal and $\\Sigma$ diagonal. The entries of $\\Sigma$ are the singular values and the columns of $U, V$ are the singular vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some pictures\n",
    "\n",
    "(1) PCA doing a good job. Points near the line $y=x$. $v_1 = x_1 + x_2, v_2 = x_1 - x_2$.\n",
    "\n",
    "(2) PCA doing a poor job. Two ellipses very close together (in $\\mathbb{R}^3$, say). First direction is the major axis of the ellipse, second direction is the minor axis, third direction will separate them. In dimension reduction context, we would try to get down to $\\mathbb{R}^2$, and we wouldn't see there are two components.\n",
    "\n",
    "Variation can be small but geometrically meaningful. PCA cares about variation, not geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDS (multidimensional scaling)\n",
    "\n",
    "Idea: preserve pairwise distances. Given similarity/distance space $X$, find $X'$ in some Euclidean space such that $\\sum_{i, j} (d(x_i, x_j) - d(x'_i, x'_j))^2$ minimized, such that (some normalization on the points in $X'$).\n",
    "\n",
    "Generalization of PCA in two ways.\n",
    "\n",
    "(1) allow a similarity matrix as input.\n",
    "\n",
    "(2) relax \"preserve pairwise distances\" to \"preserve ordering of the pairwise distances\", i.e. try to find monotonic transformation $f$ of the distances and a collection of points (in some small-ish Euclidean space) realizing that distance matrix. In notation above, change objective function to $\\sum_{i, j} (f(d(x_i, x_j) - d(x'_i, x'_j)))^2$. Note finding $f$ is not part of the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion. Both PCA and MDS are global. Except for relaxation (2) in MDS, both are essentially linear methods. Let's try to get some locality and/or non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLE (locally linear embedding)\n",
    "\n",
    "(Roweis and Saul, 2000)\n",
    "\n",
    "Preserve geometric structure of local patches, glue together in a different way to get a low-dimensional representation.\n",
    "\n",
    "(1) Express each point as a linear combination of its neighbors, so $x_i \\sum_{j=1^k} w_{ij} x_j$. Number of neighbors $k$ is a parameter choice. (This is not always possible - $x_i$ need not lie in the span of its nearest neighbors, get as close as possible by solving another optimization problem. Minimize over $w_{ij}$ the quantity $\\sum_i (x_i - \\sum_j w_{ij}x_j)^2$ such that $w_{ij} = 0$ if $x_j$ not one of $x_i$'s $k$ nearest neighbors, and subject to $\\sum_j w_{ij} =1 $ for all $i$ (normalization).\n",
    "\n",
    "(2) Find a map $F : X \\to X'$ such that $x'_i \\approx \\sum_{j=1}^k w_{ij} x'_j$ (same weights): minimize $\\sum_i (x'_i - \\sum_j w_{ij}x'_j)^2$ over $x'_i$ (with the weights fixed).\n",
    "\n",
    "So we're trying to preserve local expression of a point as a linear combination of its neighbors. Should handle the pair of ellipses well, for good choice of $k$, provided the density of the ellipses overwhelms the distance between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isomap\n",
    "\n",
    "(Tenenbaum, de Silva, Langford 2000)\n",
    "\n",
    "(1) Construct a neighborhood graph with edge between $x_i$ and $x_j$ weighted by $d(x_i, x_j)$. (Connect points to $k$ nearest neighbors, or connect a point to all points within a fixed distance $\\epsilon$. Have a parameter to tune either way.)\n",
    "\n",
    "(2) Compute shortest (weighted) paths in the graph.\n",
    "\n",
    "(3) Apply MDS to that distance matrix.\n",
    "\n",
    "Comparison to LLE: Isomap remembers global structure (since we care about all distances), but distances are forced to pass through a specified local structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
